# Label Generation

PhotoPrism’s built-in [image classification](tensorflow/index.md) relies on TensorFlow models such as Nasnet. With the new Ollama integration, you can generate labels via multimodal LLMs.

## Ollama Setup Guide

Follow the [steps in our User Guide](../../user-guide/ai/using-ollama.md) to connect PhotoPrism directly to an Ollama instance and replace (or augment) the default Nasnet classifier with labels generated by a vision-capable LLM.

[Learn more ›](../../user-guide/ai/using-ollama.md)

### Configuration Tips

PhotoPrism evaluates models from the bottom of the list up, so placing the Ollama entries after the others ensures Ollama is chosen first while the others remain available as fallback options.

Ollama-generated captions and labels are stored with the `ollama` metadata source automatically, so you do not need to request a specific `source` field in the schema or pass `--source` to the CLI unless you want to override the default.

!!! tip "Prompt Localization"
    To generate output in other languages, keep the base instructions in English and add the desired language (e.g., "Respond in German"). This method works for both [caption](../../user-guide/ai/ollama-models.md#qwen3-vl-caption) and [label prompts](../../user-guide/ai/ollama-models.md#qwen3-vl-labels).

## Troubleshooting

### Verify Active Configuration

```bash
docker compose exec photoprism photoprism vision ls
```

Ensure the output lists both Nasnet (default) and your Ollama label model. If the custom entry is missing, double-check the YAML indentation, file name (`vision.yml`, not `.yaml`), or environment overrides.

### Schema or JSON Errors

If PhotoPrism logs `vision: invalid label payload from ollama`, the model returned data that didn’t match the expected structure. Confirm that:

- The adapter injected schema instructions (keep `System`/`Prompt` intact or reuse the defaults).

PhotoPrism may fall back to the existing TensorFlow Nasnet model when the Ollama response cannot be parsed.

### Latency & Timeouts

Structured responses introduce additional parsing overhead. If you encounter timeouts:

- Increase the global service timeout (e.g., `ServiceTimeout` in advanced deployments) if needed.
- Reduce image resolution (`Resolution: 500`) or use smaller models.
- Keep `Options.Temperature` low to encourage deterministic output.

### GPU Considerations

When Ollama uses GPUs, long-running sessions might degrade over time due to VRAM fragmentation. Restart the Ollama container to recover performance:

```bash
docker compose down ollama
docker compose up -d ollama
```
